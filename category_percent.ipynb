{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook calculates the percentage of category words for a whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#lists taken from the attention parser also used to evaluate model performance\n",
    "position_words =['right', 'left', 'top', 'bottom', 'middle', 'mid', 'front', 'closest', 'nearest', 'center', 'central',\n",
    "                               'corner', 'upper', 'back', 'far', 'leftmost', 'lower', 'low', 'rightmost',\n",
    "                               'farthest', 'furthest', 'next', 'last', 'up', 'above', 'below', 'down', 'side']\n",
    "\n",
    "\n",
    "color_words = ['white', 'green', 'blue', 'red', 'yellow', 'black', 'brown', 'pink', 'dark', 'darker', 'orange',\n",
    "                            'gray', 'purple', 'beige', 'bright']\n",
    "\n",
    "size_words = [\"big\", \"bigger\", \"small\", \"smaller\", \"tall\", \"taller\", \"large\", \"larger\", \"little\", \"short\", \"shorter\",\n",
    "                           'tiny', \"long\", \"longer\", 'huge']\n",
    "\n",
    "rel_pos_words=['above', 'about', 'below',\"behind\" 'beneath', 'beside', 'between', 'by','against', \n",
    "               'from', 'through', 'under', 'underneath', 'with','near', 'inside', 'from']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(model,mode, dataset, split):\n",
    "#load predicted and gold bounding boxes\n",
    "\n",
    "    try:\n",
    "\n",
    "        #the predicted bounding box\n",
    "        with open(r\"/home/users/fschreiber/project/bboxes_\"+model+\"/\"+dataset+\"/\"+split+\"_pred_bbox_list.p\",\"rb\") as f:\n",
    "            pred_bbox_list=list(pickle.load(f))\n",
    "\n",
    "        if mode==\"non_inc\":\n",
    "            #the target bounding box\n",
    "            with open(r\"/home/users/fschreiber/project/bboxes_noninc_\"+model+\"/\"+dataset+\"/\"+split+\"_pred_bbox_list.p\",\"rb\") as f:\n",
    "                target_bbox_list=list(pickle.load(f))\n",
    "\n",
    "        elif mode == \"inc\":\n",
    "            #the target bounding box\n",
    "            with open(r\"/home/users/fschreiber/project/bboxes_\"+model+\"/\"+dataset+\"/\"+split+\"_target_bbox_list.p\",\"rb\") as f:\n",
    "                target_bbox_list=list(pickle.load(f))\n",
    "        else:\n",
    "            print(\"The mode can only be non_inc or inc\")\n",
    "            return -1,-1,-1,-1,-1\n",
    "\n",
    "        #the number of one sentence split up incrementally (\"the left zebra\" would have length 3)\n",
    "        with open(r\"/home/users/fschreiber/project/incremental_pickles/length_incremental_units/\"+dataset+\"_\"+split+\"_length_unit.p\",\"rb\") as f:\n",
    "            inc_len=pickle.load(f)\n",
    "\n",
    "        #the original model data split up incrementally\n",
    "        data_model=torch.load(\"/home/users/fschreiber/project/ready_inc_data/\"+dataset+\"/\"+dataset+\"_\"+split+\".pth\")\n",
    "\n",
    "        with open(r\"/home/users/fschreiber/project/binary_grouped/\"+model+\"/\"+mode+\"/\"+dataset+split+\".p\",\"rb\") as f:\n",
    "            binary_grouped=pickle.load(f)\n",
    "\n",
    "        \n",
    "        if mode==\"non_inc\":\n",
    "            target_bbox_list=[x for x,y in zip(target_bbox_list,inc_len) for _ in range(y)]\n",
    "            \n",
    "        if model==\"TVG\":\n",
    "            pred_bbox_list,target_bbox_list=TVG_prep(pred_bbox_list,target_bbox_list)\n",
    "                    \n",
    "\n",
    "        return pred_bbox_list,target_bbox_list,inc_len,data_model,binary_grouped\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        #print(e)\n",
    "        \n",
    "        return  -1,-1,-1,-1,-1\n",
    "    \n",
    "\n",
    "\n",
    "#TVG needs some extra adjustments to fit the same data format as Resc\n",
    "def TVG_prep(pred_bbox_list,target_bbox_list):\n",
    "    #print(\"TVG\")\n",
    "    for ind,(pred,targ) in enumerate (zip (pred_bbox_list,target_bbox_list)):\n",
    "\n",
    "        pred=pred.view(1,-1)\n",
    "\n",
    "        pred=xywh2xyxy(pred)\n",
    "        pred=torch.clamp(pred,0,1)\n",
    "\n",
    "        pred_bbox_list[ind]=pred\n",
    "\n",
    "        targ=targ.view(1,-1)\n",
    "        targ=xywh2xyxy(targ)\n",
    "\n",
    "        target_bbox_list[ind]=targ\n",
    "    return pred_bbox_list,target_bbox_list\n",
    "\n",
    "#copied from TransVG needed to transform the bounding box vectors\n",
    "def xywh2xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox_list,target_bbox_list,inc_len,model,binary_grouped=load_data(\"ReSc\",\"inc\",\"unc\",\"testB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group sentences that belong to one incremental unit\n",
    "def group_by_increment(bbox_list,inc_len):\n",
    "    counter=0\n",
    "    group_list=[]\n",
    "    for i in inc_len:\n",
    "        \n",
    "        group_list.append(bbox_list[counter:counter+i])\n",
    "        counter=counter+i\n",
    "    return group_list\n",
    "\n",
    "model_group=group_by_increment(model,inc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the sentences by word into wordcategories\n",
    "\n",
    "def count_category(model_group):\n",
    "    stop_c,pos_c,rest_c,color_c,size_c,rel_pos_c=(0 for i in range(6))\n",
    "\n",
    "    for model in model_group:\n",
    "       \n",
    "        sent=model[-1][3].split()\n",
    "        for word in sent:\n",
    "            \n",
    "            \n",
    "            if word in position_words:\n",
    "                pos_c+=1\n",
    "               \n",
    "            elif word in color_words:\n",
    "                color_c+=1\n",
    "            elif word in size_words:\n",
    "                size_c+=1\n",
    "            elif word in rel_pos_words:\n",
    "                rel_pos_c+=1\n",
    "            elif word in stopwords.words():\n",
    "                stop_c+=1    \n",
    "           \n",
    "            else:\n",
    "                rest_c+=1\n",
    "\n",
    "    return pos_c,stop_c,size_c,color_c,rel_pos_c,rest_c\n",
    "\n",
    "#pos_c,stop_c,size_c,color_c,rel_pos_c,rest_c=count_category(model_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unctestB\n",
      "unctestA\n",
      "uncval\n",
      "unc+testB\n",
      "unc+testA\n",
      "unc+val\n",
      "gref_umdval\n",
      "gref_umdtest\n",
      "grefval\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mode=\"inc\"\n",
    "model_input=\"TVG\"\n",
    "split_list=[\"testB\",\"testA\",\"val\",\"test\"]\n",
    "dataset_list=[\"unc\",\"unc+\",\"gref_umd\",\"gref\"]\n",
    "#dataset_list=[\"unc\"]\n",
    "\n",
    "pos_all,color_all,size_all,rel_pos_all,stop_all,rest_all=([] for i in range(6))\n",
    "\n",
    "\n",
    "#load the data\n",
    "for file in dataset_list:\n",
    "    for split in split_list:\n",
    "    \n",
    "        #if the data set does not exist ignore\n",
    "        pred_bbox_list,target_bbox_list,inc_len,model,binary_grouped= load_data(model_input,mode,file,split)\n",
    "        if pred_bbox_list==-1 or target_bbox_list==-1:\n",
    "             \n",
    "             pass\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            print(file+split)\n",
    "            model_group=group_by_increment(model,inc_len)\n",
    "\n",
    "            \n",
    "            pos_c,stop_c,size_c,color_c,rel_pos_c,rest_c=count_category(model_group) \n",
    "            \n",
    "            all_counter=[pos_c,stop_c,size_c,color_c,rel_pos_c,rest_c]\n",
    "\n",
    "\n",
    "            pos_all.append(round((pos_c/sum(all_counter)*100),2))\n",
    "            stop_all.append(round((stop_c/sum(all_counter)*100),2))\n",
    "            size_all.append(round((size_c/sum(all_counter)*100),2))\n",
    "            color_all.append(round((color_c/sum(all_counter)*100),2))\n",
    "            rel_pos_all.append(round((rel_pos_c/sum(all_counter)*100),2))\n",
    "            rest_all.append(round((rest_c/sum(all_counter)*100),2))\n",
    "            \n",
    "            \n",
    "df_pos_tvg=pd.DataFrame(pos_all)\n",
    "df_color_tvg=pd.DataFrame(color_all)\n",
    "df_rel_pos_tvg=pd.DataFrame(rel_pos_all)\n",
    "df_size_tvg=pd.DataFrame(size_all)\n",
    "df_stop_tvg=pd.DataFrame(stop_all)\n",
    "df_rest_tvg=pd.DataFrame(rest_all)  \n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
